{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5e48b",
   "metadata": {},
   "source": [
    "**공백 및 특수문자 처리 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc71458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    \"\"\"\n",
    "    입력된 텍스트에서 공백과 특수문자를 제거하는 함수\n",
    "    - 불필요한 공백 제거\n",
    "    - 특수문자 및 숫자 제거\n",
    "    \"\"\"\n",
    "    # 단어와 구두점(punctuation) 사이 공백\n",
    "    #text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    #text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    # 특수문자 및 숫자 제거\n",
    "    #text = re.sub('^가-힣ㄱ-하-ㅣa-zA-Z.', '', text)  # 한글, 공백만 남기고 제거\n",
    "    # 불필요한 공백 제거\n",
    "    #text = re.sub(r'\\s+', ' ', text).strip()  # 여러 공백을 하나로 치환 후 양쪽 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac35b3",
   "metadata": {},
   "source": [
    "**데이터 읽어오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6023c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ac7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../aiffel/train.csv'\n",
    "test_file_path = '../aiffel/test.csv'\n",
    "normal_file_path = '../aiffel/normal_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78698806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "normal = pd.read_csv(normal_file_path, nrows=1000) # 클래스 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de23d80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df 전체 샘플 수 : 3950\n",
      "test_df 전체 샘플 수 : 500\n",
      "normal 전체 샘플 수 : 1000\n"
     ]
    }
   ],
   "source": [
    "print('train_df 전체 샘플 수 :', len(train_df))\n",
    "print('test_df 전체 샘플 수 :', len(test_df))\n",
    "print('normal 전체 샘플 수 :', len(normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd36a7",
   "metadata": {},
   "source": [
    "데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3ff30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24661ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe61e5",
   "metadata": {},
   "source": [
    "훈련 데이터 - 라벨 정수 인코딩, 인풋 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09951f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b091f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>3945</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>3946</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>3947</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>3948</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>3949</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx        class                                       conversation  \\\n",
       "3945  3945    기타 괴롭힘 대화  준하야 넌 대가리가 왜이렇게 크냐?\\n내 머리가 뭐.\\n밥먹으면 대가리만 크냐 너는...   \n",
       "3946  3946        갈취 대화  내가 지금 너 아들 김길준 데리고 있어. 살리고 싶으면 계좌에 1억만 보내\\n예.?...   \n",
       "3947  3947  직장 내 괴롭힘 대화  나는 씨 같은 사람 보면 참 신기하더라. 어떻게 저렇게 살지.\\n왜 그래. 들리겠어...   \n",
       "3948  3948        갈취 대화  누구맘대로 여기서 장사하래?\\n이게 무슨일입니까?\\n남의 구역에서 장사하려면 자릿세...   \n",
       "3949  3949  직장 내 괴롭힘 대화  희정씨\\n네?\\n주말에 시간이 넘쳐나나봐\\n갑자기 왜그러세요?\\n손이 빤짝빤짝 네일...   \n",
       "\n",
       "      encoded_label  \n",
       "3945              3  \n",
       "3946              1  \n",
       "3947              2  \n",
       "3948              1  \n",
       "3949              2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['encoded_label'] = train_df['class'].map(label_map)\n",
    "\n",
    "print(train_df['encoded_label'].unique())\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682eb3fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 배달되나요?\\n아 네 배달됩니</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?\\n특해물 짬뽕도 있고 전복 새우...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?\\n전복 짬뽕 시키면 전복이 들어가죠\\...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>여기 #주소#인데 배달되나요?\\n#주소#는 안됩니</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중국집 명성루죠? 배달 지금 가능한가요?\\n예 배달 가능합니</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  encoded_label\n",
       "0                                지금 배달되나요?\\n아 네 배달됩니              4\n",
       "1  짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?\\n특해물 짬뽕도 있고 전복 새우...              4\n",
       "2  전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?\\n전복 짬뽕 시키면 전복이 들어가죠\\...              4\n",
       "3                        여기 #주소#인데 배달되나요?\\n#주소#는 안됩니              4\n",
       "4                  중국집 명성루죠? 배달 지금 가능한가요?\\n예 배달 가능합니              4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = normal.rename(columns={'0': 'conversation'})\n",
    "normal['encoded_label'] = 4\n",
    "normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e341b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>여기 주차장에 주차하는 거 맞아요?\\n네 5000원 이상만 하시면 2시간 찍어드려</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>이거 새로 나온 거예요?\\n새로 나온 건 핫크리스피라고 해요. 크리스피버거는 닭다리...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>뻑뻑하진 않나요 고기가?\\n닭가슴살이라 조금 퍽퍽해요\\n근데 칼로리가 좀 낮겠네요\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>어떤 게 좀 인기 있어요?\\n이게 제일 무난하게 잘 나가구요 저쪽은 치킨 종류 찾으...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>그럼 이 불고기요 버거 셋트인가요 이게?\\n이쪽은 세트고 이쪽은 버거인데요 지금 세...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx class                                       conversation  \\\n",
       "4945  NaN   NaN      여기 주차장에 주차하는 거 맞아요?\\n네 5000원 이상만 하시면 2시간 찍어드려   \n",
       "4946  NaN   NaN  이거 새로 나온 거예요?\\n새로 나온 건 핫크리스피라고 해요. 크리스피버거는 닭다리...   \n",
       "4947  NaN   NaN    뻑뻑하진 않나요 고기가?\\n닭가슴살이라 조금 퍽퍽해요\\n근데 칼로리가 좀 낮겠네요\\n   \n",
       "4948  NaN   NaN  어떤 게 좀 인기 있어요?\\n이게 제일 무난하게 잘 나가구요 저쪽은 치킨 종류 찾으...   \n",
       "4949  NaN   NaN  그럼 이 불고기요 버거 셋트인가요 이게?\\n이쪽은 세트고 이쪽은 버거인데요 지금 세...   \n",
       "\n",
       "      encoded_label  \n",
       "4945              4  \n",
       "4946              4  \n",
       "4947              4  \n",
       "4948              4  \n",
       "4949              4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, normal], axis=0, ignore_index=True)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd30b40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>친구야 저번에 돈 빌려줘서 너무 고마웠어\\n아. 응.\\n근데 내가 오늘도 깜빡하고 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어떤 게 잘 나가요?\\n이것도 취향 따라서 가는데 위트 잘 나가고</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>야 나 5만 원 빌려줘라 \\n전에도 내가 3만 원 빌려줬잖아.\\n야 내가 안 준대?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>무릎 꿇고 손 들어.\\n알겠습니다.\\n지금부터 제대로 대답하지 않으면 인질을 한 명...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>내가 싯팔 영화 스포하지 말랬지\\n.\\n오징어게임 아직 보지도 않았는데 비밀을 알아...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  encoded_label\n",
       "0  친구야 저번에 돈 빌려줘서 너무 고마웠어\\n아. 응.\\n근데 내가 오늘도 깜빡하고 ...              1\n",
       "1               어떤 게 잘 나가요?\\n이것도 취향 따라서 가는데 위트 잘 나가고              4\n",
       "2  야 나 5만 원 빌려줘라 \\n전에도 내가 3만 원 빌려줬잖아.\\n야 내가 안 준대?...              1\n",
       "3  무릎 꿇고 손 들어.\\n알겠습니다.\\n지금부터 제대로 대답하지 않으면 인질을 한 명...              0\n",
       "4  내가 싯팔 영화 스포하지 말랬지\\n.\\n오징어게임 아직 보지도 않았는데 비밀을 알아...              0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요없는 컬럼 삭제 후 데이터 셔플\n",
    "train_df = train_df.drop(columns=['idx', 'class'])\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb4f3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['conversation']\n",
    "y_train = train_df['encoded_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b267555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    친구야 저번에 돈 빌려줘서 너무 고마웠어\\n아. 응.\\n근데 내가 오늘도 깜빡하고 ...\n",
       "1                 어떤 게 잘 나가요?\\n이것도 취향 따라서 가는데 위트 잘 나가고\n",
       "2    야 나 5만 원 빌려줘라 \\n전에도 내가 3만 원 빌려줬잖아.\\n야 내가 안 준대?...\n",
       "3    무릎 꿇고 손 들어.\\n알겠습니다.\\n지금부터 제대로 대답하지 않으면 인질을 한 명...\n",
       "4    내가 싯팔 영화 스포하지 말랬지\\n.\\n오징어게임 아직 보지도 않았는데 비밀을 알아...\n",
       "Name: conversation, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504881f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    4\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: encoded_label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a37e9",
   "metadata": {},
   "source": [
    "테스트 데이터 - 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1af346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec596f31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "1    우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "2    너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "3    이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...\n",
       "4    아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176fb8b",
   "metadata": {},
   "source": [
    "# TFBertForSequenceClassification\n",
    "- Bert가 텍스트 분류 문제에서 좋은 성능을 보인다고 해서 선택했다.\n",
    "- 하지만, [데이터에 비해 모델 사이즈가 큰 탓인지] 학습이 제대로 되지 않는 문제가 발생한다.\n",
    "- 비슷한 아키텍쳐의 조금 더 작은 사이즈의 모델을 사용해봐야겠다.\n",
    "- 이후에 제대로 결과가 나오지 않는 이유를 모색해볼 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a24978",
   "metadata": {},
   "source": [
    "**토크나이징**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303d5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ee6a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BERT 모델의 토크나이저를 로드 (한국어 BERT 모델)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44c4c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d1b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dialogues(dialogues, tokenizer, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for dialogue in dialogues:\n",
    "        # 각 대화 세트를 하나의 긴 문장으로 합침\n",
    "        dialogue_text = \" [SEP] \".join(dialogue)  # 대화 턴들을 [SEP]로 구분\n",
    "        \n",
    "        # 토크나이징\n",
    "        encoded = tokenizer(\n",
    "            dialogue_text, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=max_length, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    # 텐서 형태로 변환\n",
    "    input_ids = tf.convert_to_tensor(np.array(input_ids))\n",
    "    attention_masks = tf.convert_to_tensor(np.array(attention_masks))\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "733e10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 데이터를 토크나이징\n",
    "X_train_input_ids, X_train_attention_masks = tokenize_dialogues(X_train, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdf8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 토크나이저로 토크나이즈\n",
    "#tokenized_X_train = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "#print(tokenized_X_train['input_ids'])\n",
    "#print(tokenized_X_train['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff380f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tokenized_X_test = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "#print(tokenized_X_test['input_ids'])\n",
    "#print(tokenized_X_test['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82064dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'input_ids', 'attention_mask', 'token_type_ids'를 NumPy 배열로 변환\n",
    "#X_train_input_ids = np.array(tokenized_X_train['input_ids'])\n",
    "#X_train_attention_mask = np.array(tokenized_X_train['attention_mask'])\n",
    "#X_train_token_type_ids = np.array(tokenized_X_train['token_type_ids'])\n",
    "\n",
    "#X_test_input_ids = np.array(tokenized_X_test['input_ids'])\n",
    "#X_test_attention_mask = np.array(tokenized_X_test['attention_mask'])\n",
    "#X_test_token_type_ids = np.array(tokenized_X_test['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "915845c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "X_train_enc, X_val_enc, y_train_split, y_val_split = train_test_split(\n",
    "    np.array(X_train_input_ids), y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebfc5954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_test_encodings = {\\n    'input_ids': X_test_input_ids,\\n    'attention_mask': X_test_attention_mask,\\n    #'token_type_ids': X_test_token_type_ids\\n}\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_enc와 X_val_enc에 해당하는 attention_mask, token_type_ids를 정확히 분리\n",
    "X_train_encodings = {\n",
    "    'input_ids': X_train_enc,\n",
    "    'attention_mask': [X_train_attention_masks[i] for i in range(len(X_train_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_train_enc))]\n",
    "}\n",
    "\n",
    "X_val_encodings = {\n",
    "    'input_ids': X_val_enc,\n",
    "    'attention_mask': [X_train_attention_masks[i] for i in range(len(X_val_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_val_enc))]\n",
    "}\n",
    "\n",
    "# 테스트 데이터셋의 인코딩\n",
    "'''X_test_encodings = {\n",
    "    'input_ids': X_test_input_ids,\n",
    "    'attention_mask': X_test_attention_mask,\n",
    "    #'token_type_ids': X_test_token_type_ids\n",
    "}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f37ce",
   "metadata": {},
   "source": [
    "TensorFlow Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e30d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 데이터셋 (라벨 포함)\n",
    "def create_tf_dataset(encodings, labels=None):\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(encodings),\n",
    "            labels\n",
    "        ))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))  # 라벨 없이 데이터만\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5479e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋\n",
    "train_dataset = create_tf_dataset(X_train_encodings, y_train_split).batch(4)\n",
    "\n",
    "# 검증 데이터셋\n",
    "val_dataset = create_tf_dataset(X_val_encodings, y_val_split).batch(4)\n",
    "\n",
    "# 테스트 데이터셋 (라벨 없이)\n",
    "#test_dataset = create_tf_dataset(X_test_encodings).batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933685bf",
   "metadata": {},
   "source": [
    "**모델 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3b95e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "333436af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0081caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=5e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc6340",
   "metadata": {},
   "source": [
    "**모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ec120d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:1443 call  *\n        outputs = self.bert(\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:645 call  *\n        embedding_output = self.embeddings(\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:202 call  *\n        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/normalization/layer_normalization.py:285 call\n        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/normalization/layer_normalization.py:272 _broadcast\n        return tf.reshape(v, broadcast_shape)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:196 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:8403 reshape\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 768 elements to shape [1,1,330,1] (330 elements) for '{{node tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape/ReadVariableOp, tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape/shape)' with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,330,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_372/457701101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:1443 call  *\n        outputs = self.bert(\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:645 call  *\n        embedding_output = self.embeddings(\n    /opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:202 call  *\n        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/normalization/layer_normalization.py:285 call\n        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/normalization/layer_normalization.py:272 _broadcast\n        return tf.reshape(v, broadcast_shape)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:196 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:8403 reshape\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 768 elements to shape [1,1,330,1] (330 elements) for '{{node tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape/ReadVariableOp, tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/Reshape/shape)' with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,330,1].\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d0336",
   "metadata": {},
   "source": [
    "**모델 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9547043",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 logits 추출\n",
    "logits = predictions.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits -> 예측된 클래스 (가장 높은 확률을 가진 클래스를 선택)\n",
    "pred_labels = np.argmax(logits, axis=-1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b0ecf",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30981be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAlbertForSequenceClassification, AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_Albert = AlbertTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albert 토크나이저로 토크나이즈\n",
    "tokenized_X_train = tokenizer_Albert(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "print(tokenized_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_X_test = tokenizer_Albert(X_test.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "print(tokenized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'input_ids', 'attention_mask', 'token_type_ids'를 NumPy 배열로 변환\n",
    "X_train_input_ids = np.array(tokenized_X_train['input_ids'])\n",
    "X_train_attention_mask = np.array(tokenized_X_train['attention_mask'])\n",
    "#X_train_token_type_ids = np.array(tokenized_X_train['token_type_ids'])\n",
    "\n",
    "X_test_input_ids = np.array(tokenized_X_test['input_ids'])\n",
    "X_test_attention_mask = np.array(tokenized_X_test['attention_mask'])\n",
    "#X_test_token_type_ids = np.array(tokenized_X_test['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "X_train_enc, X_val_enc, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_input_ids, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb277450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_enc와 X_val_enc에 해당하는 attention_mask, token_type_ids를 정확히 분리\n",
    "X_train_encodings = {\n",
    "    'input_ids': X_train_enc,\n",
    "    'attention_mask': [X_train_attention_mask[i] for i in range(len(X_train_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_train_enc))]\n",
    "}\n",
    "\n",
    "X_val_encodings = {\n",
    "    'input_ids': X_val_enc,\n",
    "    'attention_mask': [X_train_attention_mask[i] for i in range(len(X_val_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_val_enc))]\n",
    "}\n",
    "\n",
    "# 테스트 데이터셋의 인코딩\n",
    "X_test_encodings = {\n",
    "    'input_ids': X_test_input_ids,\n",
    "    'attention_mask': X_test_attention_mask,\n",
    "    #'token_type_ids': X_test_token_type_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋\n",
    "train_dataset = create_tf_dataset(X_train_encodings, y_train_split).batch(16)\n",
    "\n",
    "# 검증 데이터셋\n",
    "val_dataset = create_tf_dataset(X_val_encodings, y_val_split).batch(16)\n",
    "\n",
    "# 테스트 데이터셋 (라벨 없이)\n",
    "test_dataset = create_tf_dataset(X_test_encodings).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48890c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Albert = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Albert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf99ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Albert.fit(train_dataset, epochs=3, validation_data=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_Albert.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 logits 추출\n",
    "logits = predictions.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144dc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits -> 예측된 클래스 (가장 높은 확률을 가진 클래스를 선택)\n",
    "pred_labels = np.argmax(logits, axis=-1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07ccb8",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_Auto = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list, max_seq_len, tokenizer):\n",
    "    input_ids = []\n",
    "\n",
    "    for conversation in tqdm(sent_list, total=len(sent_list)):\n",
    "        bos_token = [tokenizer.bos_token]  # 시작 토큰\n",
    "        eos_token = [tokenizer.eos_token]  # 종료 토큰\n",
    "        unused_token = ['<unused0>']  # 마지막에 추가할 토큰\n",
    "\n",
    "        # '\\n'으로 문장들 분리\n",
    "        sentences = conversation.split(\"\\n\")\n",
    "\n",
    "        tokens = []  # 전체 토큰 리스트\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # 각 문장에 <s>와 </s> 추가\n",
    "            sentence_tokens = bos_token + tokenizer.tokenize(sentence) + eos_token\n",
    "            tokens += sentence_tokens  # 문장 뒤에 <s> 추가\n",
    "\n",
    "        # 마지막 문장 뒤에 <unused0> 추가\n",
    "        tokens += unused_token  # 마지막에 <unused0> 추가\n",
    "        tokens += eos_token  # 마지막에 종료 토큰 추가\n",
    "\n",
    "        # 정수 인코딩\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # 패딩 추가\n",
    "        input_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding='post')[0]\n",
    "\n",
    "        # 길이 확인\n",
    "        assert len(input_id) == max_seq_len, f\"Error with input length {len(input_id)} vs {max_seq_len}\"\n",
    "\n",
    "        # 결과 저장\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_X_train = convert_examples_to_features(X_train, max_seq_len=max_length, tokenizer=tokenizer_Auto)\n",
    "converted_X_test = convert_examples_to_features(X_test, max_seq_len=max_length, tokenizer=tokenizer_Auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce92cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_input_ids, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13231934",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "outputs = model([input_ids_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2ForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TFGPT2ForSequenceClassification, self).__init__()\n",
    "        self.gpt = TFGPT2Model.from_pretrained(model_name, from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt(input_ids=inputs)\n",
    "        cls_token = outputs[0][:, -1]\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd792b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GPT2 = TFGPT2ForSequenceClassification(\"skt/kogpt2-base-v2\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5393d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ec804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, epochs=3, batch_size=4, validation_data = (X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedae8be",
   "metadata": {},
   "source": [
    "# hgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "242d7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e04b41f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(4950, 330), dtype=int32, numpy=\n",
      "array([[    2,  9466,  5059, ...,     0,     0,     0],\n",
      "       [    2,  8879,  1977, ...,     0,     0,     0],\n",
      "       [    2,  3545,  2239, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    2, 10412, 10256, ...,     0,     0,     0],\n",
      "       [    2, 16647,  3805, ...,     0,     0,     0],\n",
      "       [    2,  2239, 16835, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(4950, 330), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4950, 330), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저로 토크나이즈\n",
    "tokenized_X_train = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "print(tokenized_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72a19a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(500, 330), dtype=int32, numpy=\n",
      "array([[    2, 17613,  5563, ...,     0,     0,     0],\n",
      "       [    2,  8472,  5334, ...,     0,     0,     0],\n",
      "       [    2,  2276,  9062, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    2,  3545,  2276, ...,     0,     0,     0],\n",
      "       [    2,  3545,  2276, ...,     0,     0,     0],\n",
      "       [    2,  9686,  3805, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(500, 330), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(500, 330), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "tokenized_X_test = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
    "\n",
    "print(tokenized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08842c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'input_ids', 'attention_mask', 'token_type_ids'를 NumPy 배열로 변환\n",
    "X_train_input_ids = np.array(tokenized_X_train['input_ids'])\n",
    "X_train_attention_mask = np.array(tokenized_X_train['attention_mask'])\n",
    "X_train_token_type_ids = np.array(tokenized_X_train['token_type_ids'])\n",
    "\n",
    "X_test_input_ids = np.array(tokenized_X_test['input_ids'])\n",
    "X_test_attention_mask = np.array(tokenized_X_test['attention_mask'])\n",
    "X_test_token_type_ids = np.array(tokenized_X_test['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "836c23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "X_train_enc, X_val_enc, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_input_ids, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85d6aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_enc와 X_val_enc에 해당하는 attention_mask, token_type_ids를 정확히 분리\n",
    "X_train_encodings = {\n",
    "    'input_ids': X_train_enc,\n",
    "    'attention_mask': [X_train_attention_masks[i] for i in range(len(X_train_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_train_enc))]\n",
    "}\n",
    "\n",
    "X_val_encodings = {\n",
    "    'input_ids': X_val_enc,\n",
    "    'attention_mask': [X_train_attention_masks[i] for i in range(len(X_val_enc))],\n",
    "    #'token_type_ids': [X_train_token_type_ids[i] for i in range(len(X_val_enc))]\n",
    "}\n",
    "\n",
    "# 테스트 데이터셋의 인코딩\n",
    "X_test_encodings = {\n",
    "    'input_ids': X_test_input_ids,\n",
    "    'attention_mask': X_test_attention_mask,\n",
    "    #'token_type_ids': X_test_token_type_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f171050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 데이터셋 (라벨 포함)\n",
    "def create_tf_dataset(encodings, labels=None):\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(encodings),\n",
    "            labels\n",
    "        ))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))  # 라벨 없이 데이터만\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c388f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋\n",
    "train_dataset = create_tf_dataset(X_train_encodings, y_train_split).batch(4)\n",
    "\n",
    "# 검증 데이터셋\n",
    "val_dataset = create_tf_dataset(X_val_encodings, y_val_split).batch(4)\n",
    "\n",
    "# 테스트 데이터셋 (라벨 없이)\n",
    "test_dataset = create_tf_dataset(X_test_encodings).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e39abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/snunlp/KR-FinBert-SC/resolve/main/tf_model.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load weights for 'snunlp/KR-FinBert-SC'. Make sure that:\n\n- 'snunlp/KR-FinBert-SC' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'snunlp/KR-FinBert-SC' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[1;32m   1393\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1403\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/snunlp/KR-FinBert-SC/resolve/main/tf_model.h5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_372/1214461776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"snunlp/KR-FinBert-SC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         raise ValueError(\n\u001b[1;32m    421\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                     \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 )\n\u001b[0;32m-> 1409\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading weights file {archive_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load weights for 'snunlp/KR-FinBert-SC'. Make sure that:\n\n- 'snunlp/KR-FinBert-SC' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'snunlp/KR-FinBert-SC' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert-SC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327fdc6c",
   "metadata": {},
   "source": [
    "**데이터 전처리에 문제 있는 것이 맞는듯**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659adc4",
   "metadata": {},
   "source": [
    "---\n",
    "# 내보내기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2625351",
   "metadata": {},
   "source": [
    "**예측 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751617a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 라벨을 test_df에 컬럼으로 추가\n",
    "test_df['class'] = pred_labels\n",
    "\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['text'])\n",
    "\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35bdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 submission.csv로 저장 (index는 제외)\n",
    "test_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbbf89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
